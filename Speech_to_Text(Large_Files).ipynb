{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech to Text(Large Files).ipynb",
      "provenance": [],
      "mount_file_id": "1ynlhfsBpfmzzsLhDI8ulcOWkueMndU7C",
      "authorship_tag": "ABX9TyN+SONOPuzRzyfx03AM0b3g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raries16/Audio-Text/blob/main/Speech_to_Text(Large_Files).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvXQTb7Yg_f3",
        "outputId": "127f8d6f-aa52-484b-d039-45b9faa4cacd"
      },
      "source": [
        "pip install SpeechRecognition"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.6/dist-packages (3.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acSu5rvPrnWn",
        "outputId": "ef6cbcf2-f649-4d0b-d98d-3aeaa3b9606a"
      },
      "source": [
        "pip install pydub"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydub\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/d1/fbfa79371a8cd9bb15c2e3c480d7e6e340ed5cc55005174e16f48418333a/pydub-0.24.1-py2.py3-none-any.whl\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.24.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi0UgUY80Hs5",
        "outputId": "b69f082c-3cec-44ed-af9d-b22e9264d41b"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtethYLKhEzP"
      },
      "source": [
        "import speech_recognition as sr \r\n",
        "import os \r\n",
        "from pydub import AudioSegment\r\n",
        "from pydub.silence import split_on_silence"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9mN2HXgSzK8"
      },
      "source": [
        "AUDIO_FILE = ('/content/drive/MyDrive/Colab_Notebooks/Training.wav')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbA_Y0IzkyFK"
      },
      "source": [
        "# create a speech recognition object\r\n",
        "r = sr.Recognizer()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0iXgPGxlG5l"
      },
      "source": [
        "# a function that splits the audio file into chunks\r\n",
        "# and applies speech recognition\r\n",
        "def get_large_audio_transcription(path):\r\n",
        "    \"\"\"\r\n",
        "    Splitting the large audio file into chunks\r\n",
        "    and apply speech recognition on each of these chunks\r\n",
        "    \"\"\"\r\n",
        "    # open the audio file using pydub\r\n",
        "    sound = AudioSegment.from_wav(path)  \r\n",
        "    # split audio sound where silence is 700 miliseconds or more and get chunks\r\n",
        "    chunks = split_on_silence(sound,\r\n",
        "        # experiment with this value for your target audio file\r\n",
        "        min_silence_len = 500,\r\n",
        "        # adjust this per requirement\r\n",
        "        silence_thresh = sound.dBFS-14,\r\n",
        "        # keep the silence for 1 second, adjustable as well\r\n",
        "        keep_silence=500,\r\n",
        "    )\r\n",
        "    folder_name = \"audio-chunks\"\r\n",
        "    # create a directory to store the audio chunks\r\n",
        "    if not os.path.isdir(folder_name):\r\n",
        "        os.mkdir(folder_name)\r\n",
        "    whole_text = \"\"\r\n",
        "    # process each chunk \r\n",
        "    for i, audio_chunk in enumerate(chunks, start=1):\r\n",
        "        # export audio chunk and save it in\r\n",
        "        # the `folder_name` directory.\r\n",
        "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\r\n",
        "        audio_chunk.export(chunk_filename, format=\"wav\")\r\n",
        "        # recognize the chunk\r\n",
        "        with sr.AudioFile(chunk_filename) as source:\r\n",
        "            audio_listened = r.record(source)\r\n",
        "            # try converting it to text\r\n",
        "            try:\r\n",
        "                text = r.recognize_google(audio_listened)\r\n",
        "            except sr.UnknownValueError as e:\r\n",
        "                print(\"Error:\", str(e))\r\n",
        "            else:\r\n",
        "                text = f\"{text.capitalize()}. \"\r\n",
        "                print(chunk_filename, \":\", text)\r\n",
        "                whole_text += text\r\n",
        "    # return the text for all chunks detected\r\n",
        "    return whole_text"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkmUFE3CsBTu",
        "outputId": "4ef99a0c-9efa-4841-af65-052517f5afa1"
      },
      "source": [
        "print(\"\\nFull text:\", get_large_audio_transcription(AUDIO_FILE))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: \n",
            "audio-chunks/chunk2.wav : Hi everybody laurence maroney here i'm at tensorflow world and we've just come from the keynote that was given by jeff dean and i saw jeff well, thanks for coming to talk with us lots of great contents so many things that we don't have time to go over them all but there was one really impactful thing that i saw and you were talking about liking computer vision the way now the error rates in humans is like 5% in computer vision and now with machines is down to 3%. \n",
            "audio-chunks/chunk3.wav : But i do think the progress we've made from about 26% error in 2011 down to 3% in 2016 is. \n",
            "audio-chunks/chunk4.wav : Hugely impactful because. \n",
            "audio-chunks/chunk5.wav : The way i like to think about it is computers have now of all guys that work right and so we've now got the ability for computers to perceive the world around them in ways that the didn't exist six or seven years ago and opened up applications of computing. just didn't exist before you can depend on. \n",
            "audio-chunks/chunk6.wav : Radiologist labeling x-rays or ophthalmologist labeling i images and then you train a computer vision model on that task whatever it might be you can now for to replicate the expertise of those those domain experts. \n",
            "audio-chunks/chunk7.wav : In a way that makes it possible to bring and deploy that that's for an expertise much more wi-fi you can get something onto a gpu card and do a hundred images of second inaugural village all over the world. \n",
            "audio-chunks/chunk8.wav : That's right yeah so you can offer if you have clinicians were already doing this asked you can offer them like an instant second opinion like a second colleague they can turn to. \n",
            "audio-chunks/chunk9.wav : But you can also deploy it in places where there are just aren't enough doctors. \n",
            "audio-chunks/chunk10.wav : Get from reading a book. \n",
            "audio-chunks/chunk11.wav : But understanding. \n",
            "audio-chunks/chunk12.wav : Two paragraphs of text is actually a pretty fundamentally fundamentally useful thing for all kinds of things i can use be used to improve our search to suddenly or just last week we announce the use of a burke model which is a sort of. \n",
            "audio-chunks/chunk13.wav : Fairly sophisticated natural language processing model. \n",
            "audio-chunks/chunk14.wav : In the middle of our search ranking algorithms and that's been shown to improve our search results quite a lot for. \n",
            "audio-chunks/chunk15.wav : Lots of different kinds of queries that were previously pretty hard cool cool and. \n",
            "audio-chunks/chunk16.wav : But we usually start from nothing without model we basically initialize the framers of model with random floating point numbers and then try to learn everything about that task. \n",
            "audio-chunks/chunk17.wav : From the dataset we collected and that seems pretty unrealistic and sort of. \n",
            "audio-chunks/chunk18.wav : Akin to like when you want to learn to do something do you forget all your education and you go and now you try to learn everything about this house and that's going to require that you have a lot more examples of what it is you trying to do because you're not generalizing from all the other things you already know how to do and it's. \n",
            "audio-chunks/chunk19.wav : Attached eve good. \n",
            "audio-chunks/chunk20.wav : Good outcomes in a if instead you had a model that knew how to do lots and lots of things to do in the limit all the things were training separate machine learning models for why aren't we training one large model for this with different pieces of expertise that are you i think it's really important that if we have a large model that we only sort of sparsely activate we call upon different pieces of it as needed but mostly you know 99% of the models idol for any given task. \n",
            "audio-chunks/chunk21.wav : There's a lot of interesting machine learning research questions how do we do to have a model that involves it structure that like learn to route two different pieces of the models that are most appropriate but i'm pretty excited.. \n",
            "\n",
            "Full text: Hi everybody laurence maroney here i'm at tensorflow world and we've just come from the keynote that was given by jeff dean and i saw jeff well, thanks for coming to talk with us lots of great contents so many things that we don't have time to go over them all but there was one really impactful thing that i saw and you were talking about liking computer vision the way now the error rates in humans is like 5% in computer vision and now with machines is down to 3%. But i do think the progress we've made from about 26% error in 2011 down to 3% in 2016 is. Hugely impactful because. The way i like to think about it is computers have now of all guys that work right and so we've now got the ability for computers to perceive the world around them in ways that the didn't exist six or seven years ago and opened up applications of computing. just didn't exist before you can depend on. Radiologist labeling x-rays or ophthalmologist labeling i images and then you train a computer vision model on that task whatever it might be you can now for to replicate the expertise of those those domain experts. In a way that makes it possible to bring and deploy that that's for an expertise much more wi-fi you can get something onto a gpu card and do a hundred images of second inaugural village all over the world. That's right yeah so you can offer if you have clinicians were already doing this asked you can offer them like an instant second opinion like a second colleague they can turn to. But you can also deploy it in places where there are just aren't enough doctors. Get from reading a book. But understanding. Two paragraphs of text is actually a pretty fundamentally fundamentally useful thing for all kinds of things i can use be used to improve our search to suddenly or just last week we announce the use of a burke model which is a sort of. Fairly sophisticated natural language processing model. In the middle of our search ranking algorithms and that's been shown to improve our search results quite a lot for. Lots of different kinds of queries that were previously pretty hard cool cool and. But we usually start from nothing without model we basically initialize the framers of model with random floating point numbers and then try to learn everything about that task. From the dataset we collected and that seems pretty unrealistic and sort of. Akin to like when you want to learn to do something do you forget all your education and you go and now you try to learn everything about this house and that's going to require that you have a lot more examples of what it is you trying to do because you're not generalizing from all the other things you already know how to do and it's. Attached eve good. Good outcomes in a if instead you had a model that knew how to do lots and lots of things to do in the limit all the things were training separate machine learning models for why aren't we training one large model for this with different pieces of expertise that are you i think it's really important that if we have a large model that we only sort of sparsely activate we call upon different pieces of it as needed but mostly you know 99% of the models idol for any given task. There's a lot of interesting machine learning research questions how do we do to have a model that involves it structure that like learn to route two different pieces of the models that are most appropriate but i'm pretty excited.. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OQf9OsbsHnw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}